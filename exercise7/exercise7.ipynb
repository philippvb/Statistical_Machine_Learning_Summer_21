{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "4b5be0c7d8816b0411dd8381c01b6ac4b538687487d40264816f7c3f94b5666c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### (a)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\\begin{align*}\n",
    "  \\langle \\Phi_m(x), \\Phi_m(y) \\rangle\n",
    "  &= \\sum_{m \\in M} \\Phi_m(x), \\Phi_m(y)\\\\\n",
    "  &= \\sum_{m \\in M} \\sqrt{\\frac{d!}{\\prod_i m_i!}} \\prod_i x_i^{m_i} \\cdot \\sqrt{\\frac{d!}{\\prod_i m_i!}} \\prod_i y_i^{m_i}\\\\\n",
    "  &= \\sum_{m \\in M} \\sqrt{\\frac{d!}{\\prod_i m_i!}} (\\prod_i x_i^{m_i}  \\prod_i y_i^{m_i})\\\\\n",
    "  &= \\sum_{m \\in M} \\sqrt{\\frac{d!}{\\prod_i m_i!}} \\prod_i x_i^{m_i}y_i^{m_i}\\\\\n",
    "  &= \\sum_{m \\in M} \\sqrt{\\frac{d!}{\\prod_i m_i!}} \\prod_i (x_i \\cdot y_i)^{m_i}\\\\\n",
    "  &= (x_1 \\cdot y_1 + \\dots + x_n \\cdot y_n)^d\\\\\n",
    "  &= \\langle x, y\\rangle^d\n",
    "\\end{align*}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Basic sketch:\n",
    "\n",
    "The multinomial sum consists of m+d-1 over d-1 different terms, different basis functions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Exercise 12"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\\begin{align*}\n",
    "  R(f') \n",
    "  &= R(f) R(f') - R(f)\\\\\n",
    "  &\\leq R(f) \\vert R(f') - R(f) \\vert \\\\\n",
    "  &\\leq R(f) \\cdot 2 \\sup_{f \\in F} \\vert \\hat{R}_m(f) - R(f) \\vert \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "  P(\\vert \\hat{R}_m(f) - R(f) \\vert \\geq \\epsilon)\n",
    "  &= P(\\vert \\frac{1}{m} \\sum_i \\hat{R}_i(f) - R(f) \\vert\\geq \\epsilon )\\\\\n",
    "  &\\leq 2 \\exp{-\\frac{2m \\epsilon^2}{(b-a)^2}}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Now solve for delta\n",
    "\n",
    "\\begin{align*}\n",
    "  \\delta &= 4n \\exp{-\\frac{2m \\epsilon^2}{(b-a)^2}}\\\\\n",
    "  \\frac{\\delta}{4n} &= \\exp{-\\frac{2m \\epsilon^2}{(b-a)^2}}\\\\\n",
    "  \\log{\\frac{\\delta}{4n}} &= -\\frac{2m \\epsilon^2}{(b-a)^2}\\\\\n",
    "  \\log{\\frac{4n}{\\delta}} &= \\frac{2m \\epsilon^2}{(b-a)^2}\\\\\n",
    "  \\frac{1}{2m}\\log{\\frac{4n}{\\delta}} &= \\frac{\\epsilon^2}{(b-a)^2}\\\\\n",
    "  \\sqrt{\\frac{1}{2m}\\log{\\frac{4n}{\\delta}}} &= \\frac{\\epsilon}{(b-a)}\\\\\n",
    "\\end{align*}\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from scipy.linalg import cho_factor, cho_solve\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"diabetes_data.npy\", allow_pickle=True).item()\n",
    "X_train = data[\"Xtrain\"]\n",
    "Y_train = data[\"Ytrain\"]\n",
    "X_test = data[\"Xtest\"]\n",
    "Y_test = data[\"Ytest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x, y, mu):\n",
    "    return np.exp(- mu * cdist(x,y)**2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss(Y, Y_prediction):\n",
    "    return 0.5* norm(Y == np.sign(Y_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, lambda_reg, mu):\n",
    "    n = X.shape[0]\n",
    "    K = gaussian_kernel(X, X, mu)\n",
    "    alpha = cho_solve(cho_factor(K + n * lambda_reg * np.eye(n)), Y)\n",
    "    train_loss = Loss(Y, predict(X, X, alpha, mu))\n",
    "    return alpha, train_loss\n",
    "\n",
    "def predict(X_pred, X_t, alpha, mu):\n",
    "    return np.sum(np.multiply(np.squeeze(alpha), gaussian_kernel(X_pred, X_t, mu)), axis=1)\n",
    "    \n",
    "def test(X_test, Y_test, X, alpha, mu):\n",
    "    return Loss(Y_test, predict(X_test, X, alpha, mu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossValidation(X, Y, lambdas, mus, k=5):\n",
    "    X_split = np.array(np.array_split(X, k))\n",
    "    Y_split = np.array(np.array_split(Y, k))\n",
    "    min_loss=np.inf\n",
    "    min_mu = None \n",
    "    min_lambda = None\n",
    "    for mu in mus:\n",
    "        for l in lambdas:\n",
    "            # compute k-fold cross validation\n",
    "            validation_loss = []\n",
    "            for i in range(k):\n",
    "                x_train = np.concatenate(np.delete(X_split, i, axis=0))\n",
    "                y_train = np.concatenate(np.delete(Y_split, i, axis=0))\n",
    "                x_val = X_split[i]\n",
    "                y_val = Y_split[i]\n",
    "                alphas, train_loss = train(x_train, y_train, l, mu)\n",
    "                validation_loss.append(test(x_val, y_val, x_train, alphas, mu)/x_val.shape[0]) # normalized by size of xval\n",
    "            validation_loss = np.mean(validation_loss)\n",
    "            print(f\"The average loss for mu={mu}, lambda={l} is {validation_loss}\")\n",
    "            # check if smaller\n",
    "            if validation_loss < min_loss:\n",
    "                min_loss = validation_loss\n",
    "                min_mu = mu\n",
    "                min_lambda = l\n",
    "                print(f\"Found new best parameters mu={min_mu}, lambda={min_lambda}\")\n",
    "    # finally train on the whole set again\n",
    "    return train(X, Y, min_lambda, min_mu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The average loss for mu=0.0001, lambda=0.0001 is 0.39796057467662804\n",
      "Found new best parameters mu=0.0001, lambda=0.0001\n",
      "The average loss for mu=0.0001, lambda=0.001 is 0.40885305934927574\n",
      "The average loss for mu=0.0001, lambda=0.01 is 0.40956260094978514\n",
      "The average loss for mu=0.0001, lambda=0.1 is 0.40956260094978514\n",
      "The average loss for mu=0.0001, lambda=1 is 0.40956260094978514\n",
      "The average loss for mu=0.001, lambda=0.0001 is 0.39506631148311\n",
      "Found new best parameters mu=0.001, lambda=0.0001\n",
      "The average loss for mu=0.001, lambda=0.001 is 0.39796057467662804\n",
      "The average loss for mu=0.001, lambda=0.01 is 0.40885305934927574\n",
      "The average loss for mu=0.001, lambda=0.1 is 0.40956260094978514\n",
      "The average loss for mu=0.001, lambda=1 is 0.40956260094978514\n",
      "The average loss for mu=0.01, lambda=0.0001 is 0.39391449353952573\n",
      "Found new best parameters mu=0.01, lambda=0.0001\n",
      "The average loss for mu=0.01, lambda=0.001 is 0.3947496429036761\n",
      "The average loss for mu=0.01, lambda=0.01 is 0.3983654123235557\n",
      "The average loss for mu=0.01, lambda=0.1 is 0.40885305934927574\n",
      "The average loss for mu=0.01, lambda=1 is 0.40956260094978514\n",
      "The average loss for mu=0.1, lambda=0.0001 is 0.38411781803073863\n",
      "Found new best parameters mu=0.1, lambda=0.0001\n",
      "The average loss for mu=0.1, lambda=0.001 is 0.39205949017760655\n",
      "The average loss for mu=0.1, lambda=0.01 is 0.39424877876455955\n",
      "The average loss for mu=0.1, lambda=0.1 is 0.4008111233865523\n",
      "The average loss for mu=0.1, lambda=1 is 0.40956260094978514\n",
      "The average loss for mu=1, lambda=0.0001 is 0.3853597407579482\n",
      "The average loss for mu=1, lambda=0.001 is 0.38615528809882116\n",
      "The average loss for mu=1, lambda=0.01 is 0.39164593547902704\n",
      "The average loss for mu=1, lambda=0.1 is 0.3957192365599115\n",
      "The average loss for mu=1, lambda=1 is 0.39688928761227077\n"
     ]
    }
   ],
   "source": [
    "mus = [1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "lambdas = [1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "best_alphas, min_test_loss = CrossValidation(X_train, Y_train, lambdas, mus, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "95.33362470817943"
      ]
     },
     "metadata": {},
     "execution_count": 209
    }
   ],
   "source": [
    "min_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}