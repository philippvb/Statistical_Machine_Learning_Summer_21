\newcommand{\NUMBER}{1}
\newcommand{\EXERCISES}{3}
\newcommand{\DEADLINE}{09.11.2020}
\newcommand{\COURSE}{Statistical Machine Learning}
\newcommand{\STUDENTA}{Philipp von Bachmann}
\newcommand{\STUDENTB}{Laura HÃ¤ge}
\documentclass[a4paper]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{bbm}
\usepackage{amsmath, enumerate, amssymb, multirow, fancyhdr, color, graphicx, lastpage, listings, tikz, pdflscape, subfigure, float, polynom, hyperref, tabularx, forloop, geometry, listings, fancybox, tikz, forest, tabstackengine, cancel}
\input kvmacros
\geometry{a4paper,left=3cm, right=3cm, top=3cm, bottom=3cm}
\pagestyle {fancy}
\fancyhead[C]{\COURSE}
\fancyhead[R]{\today}
\fancyfoot[L]{}
\fancyfoot[C]{}
\fancyfoot[R]{Page \thepage /\pageref*{LastPage}}
\def\header#1#2{
  \begin{center}
    {\Large Assignment #1}\\
    %{(Due by: #2)}
  \end{center}
}

\newcounter{punktelistectr}
\newcounter{punkte}
\newcommand{\punkteliste}[2]{%
  \setcounter{punkte}{#2}%
  \addtocounter{punkte}{-#1}%
  \stepcounter{punkte}%<-- also punkte = m-n+1 = Anzahl Spalten[1]
  \begin{center}%
  \begin{tabularx}{\linewidth}[]{@{}*{\thepunkte}{>{\centering\arraybackslash} X|}@{}>{\centering\arraybackslash}X}
      \forloop{punktelistectr}{#1}{\value{punktelistectr} < #2 } %
      {%
        \thepunktelistectr &
      }
      #2 &  $\Sigma$ \\
      \hline
      \forloop{punktelistectr}{#1}{\value{punktelistectr} < #2 } %
      {%
        &
      } &\\
      \forloop{punktelistectr}{#1}{\value{punktelistectr} < #2 } %
      {%
        &
      } &\\
    \end{tabularx}
  \end{center}
}
\begin{document}

\begin{tabularx}{\linewidth}{m{0.3 \linewidth}X}
  \begin{minipage}{\linewidth}
    \STUDENTA\\
    \STUDENTB
  \end{minipage} & \begin{minipage}{\linewidth}
    \punkteliste{1}{\EXERCISES}
  \end{minipage}\\
\end{tabularx}
\header{Nr. \NUMBER}{\DEADLINE}

\section{Bayes Optimal Function}

  \begin{align*}
      & \frac{\partial }{\partial c(x)} \int_y (log(c(x)) + \frac{y}{c(x)}) p(y\vert x) dy \\
      &=  \int_y (\frac{\partial }{\partial c(x)} log(c(x)) + \frac{\partial }{\partial c(x)} \frac{y}{c(x)}) p(y\vert x) dy \\
      &=  \int_y (\frac{1}{c(x)} + y \cdot (-\frac{1}{c(x)^2})) p(y\vert x) dy \\
      &=  \frac{1}{c(x)} \int_y ( 1 - y \cdot \frac{1}{c(x)}) p(y\vert x) dy \\
      &=  \frac{1}{c(x)} (\int_y p(y\vert x) dy - \int_y  y \cdot \frac{1}{c(x)} p(y\vert x) dy) \\
      &=  \frac{1}{c(x)} (1- \frac{1}{c(x)} \cdot \int_y  y \cdot p(y\vert x) dy) \\
  \end{align*}

  Now set to 0:

  \begin{align*}
    &0 =  \frac{1}{c(x)} (1- \frac{1}{c(x)} \cdot \int_y  y \cdot p(y\vert x) dy) \\
    & \text{because } \frac{1}{c(x)} > 0,  x \in \mathbb{R}:\\
    \Rightarrow &0 =  (1- \frac{1}{c(x)} \cdot \int_y  y \cdot p(y\vert x) dy) \\
    &1 = \frac{1}{c(x)} \cdot \int_y  y \cdot p(y\vert x) dy) \\
    &c(x) =  \int_y  y \cdot p(y\vert x) dy) = E[Y \vert X] \\
  \end{align*}


\section{Bayes Optimal Function}


  \subsection*{(a)}
  \begin{align*}
    E[L(Y)|X]
    &= max(0, 1-f(x)) P(Y=-1|X) + (1 + kf(x)) P(Y=-1|X) \\&+ max(0, 1-f(x)) P(Y=1|X) + (1 + kf(x)) P(Y=1|X)\\
    &= (max(0, 1-f(x)) + (1 + kf(x))) \cdot (P(Y=-1|X) + P(Y=1|X))\\
    &= (max(0, 1-f(x)) + (1 + kf(x))) \cdot 1\\
  \end{align*}
  We see that this expression gets maximized for $f(x) = 0$, as $k\geq 1$.


  % \begin{align*}
  %   E[L(Y)|X]
  %   &= max(0, 1-f(x)) P(Y=-1|X)P(X=-1) + (1 + kf(x)) P(Y=-1|X) P(X=1)\\&+ max(0, 1-f(x)) P(Y=1|X) P(X=1)+ (1 + kf(x)) P(Y=1|X)P(X=-1)\\
  %   &= max(0, 1-f(x)) (P(Y=-1|X)P(X=-1) + P(Y=1|X)P(X=1)) \\&+ (1 + kf(x)) (P(Y=-1|X)P(X=1) + P(Y=1|X)P(X=-1))\\
  %   &= (max(0, 1-f(x)) + 1 - (1 + kf(x))) * (2 P(Y=1| X) P(X=1) - P(X=1) - P(Y=1| X) )\\
  %   &= (max(0, 1-f(x)) + kf(x))) * (2 P(Y=1| X) P(X=1) - P(X=1) - P(Y=1| X) )\\
  % \end{align*}

  % P * P(x) + (1-p) * P(-x)
  % = P * P(x) + (1-p) * (1-P(x))
  % = P * P(x) + 1 - p - p(x) + P * P(x)
  % = 2P * P(X) + 1 - P - P(x)

  % P * P(-x) + (1-p) * P(x)
  % = P (1-P(X)) + (1-p) * P(x)
  % = P - P * P(X) + P(X) - P * P(x)
  % = -2P P(X) + P + P(X)



  \subsubsection*{(b)}
  As we have seen in (a), the best function always predicts 0. As 0 is
  undefined, if we define it one class or another, it will always predict that
  class and is therefore not classification calibrated. Another way to see this
  is that the derivative at 0 doesn't exits, as the right side derivative is 1
  where the left side derivative is k and k>1.

\section{Bayes error}
  \subsection*{(a)}

    \begin{align*}
      R^* &= min(E_X[\mathbbm{1}_{f(x)= 1} P(Y=-1, X) + \mathbbm{1}_{f(x)= -1} P(Y=1, X)]\\
      & = min(\int_x [\mathbbm{1}_{f(x)= 1} P(Y=-1, X) + \mathbbm{1}_{f(x)= -1} P(Y=1, X)])\\
      & = min(\int_{x \in [0, \frac{1}{8}]}[\mathbbm{1}_{f(x)= 1} P(Y=-1, X) + \mathbbm{1}_{f(x)= -1} P(Y=1, X)]\\
      & + \int_{x \in [\frac{1}{8}, \frac{7}{8}]}[\mathbbm{1}_{f(x)= 1} P(Y=-1, X) + \mathbbm{1}_{f(x)= -1} P(Y=1, X)]\\
      & + \int_{x \in [\frac{7}{8}, 1]}[\mathbbm{1}_{f(x)= 1} P(Y=-1, X) + \mathbbm{1}_{f(x)= -1} P(Y=1, X)])\\
    \end{align*}

    
    all three terms can be minimized on their own, and we get the optimal classifier as:
$$f(x) = \begin{cases}
      -1 \text{ if }  0 \leq x \leq \frac{1}{8}\\
      1 \text{ if }  \frac{1}{8} < x \leq \frac{7}{8}\\
      -1 \text{ if }  \frac{7}{8} < x \leq 1\\
    \end{cases}$$
    This results in the error:
    $R^* =  \frac{1}{8} \cdot 0.1 + \frac{6}{8} \cdot 0.1 + \frac{1}{8} \cdot 0.1 = 0.1$


  \subsection*{(b)}
    If we set $b = -\frac{1}{8}$, we see that we get the best (meaning same as
    in (a)) predictions for $X \in [0, \frac{7}{8}]$. As we can only linearly
    seperate the space $X$ with the given classifier, we can not improve the class
    separation and the Loss upon that. As we have 0-1 Loss, we can now choose $w \in
    \mathbb{R}_{\ge 0}$ arbitrary.











\end{document}