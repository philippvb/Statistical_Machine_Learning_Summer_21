{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python374jvsc74a57bd0c2e66ee886b16deb6c5c6a69d669949c46fb67f048777418ad8d7650dc5ed882",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg\n",
    "\n",
    "data = np.load(\"digits389.npy\", allow_pickle=True).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data[\"Xtrain\"]\n",
    "Y_train = data[\"Ytrain\"]\n",
    "X_test = data[\"Xtest\"]\n",
    "Y_test = data[\"Ytest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_feature(X,Y):\n",
    "    y_indices = np.array([np.squeeze(np.where(y_i == np.unique(Y))) for y_i in Y])\n",
    "    mu = np.mean(X, axis=0)\n",
    "    mu_class = np.array([np.mean(X[Y==k, :], axis=0) for k in np.unique(Y)])\n",
    "    within = np.sum([np.outer(X[index, :] - mu_class[y_index], X[index, :] - mu_class[y_index]) for index, y_index in enumerate(y_indices)], axis=0)\n",
    "    # within = np.cov(X.T, x)\n",
    "\n",
    "    between_class = np.sum([np.count_nonzero(Y==y_i)* np.outer(mu_class_i - mu, mu_class_i - mu) for mu_class_i, y_i in zip(mu_class, np.unique(Y))])\n",
    "    eigen_values, eigen_vectors = np.linalg.eig(np.linalg.inv(within).dot(between_class))\n",
    "    # eigen_values, eigen_vectors = scipy.linalg.eig(between_class, within)\n",
    "    eigen_vectors = eigen_vectors[np.argsort(eigen_values), :]\n",
    "    eigen_vectors = eigen_vectors[-(len(np.unique(Y))-1):, :].T\n",
    "    return eigen_vectors, X @ eigen_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors, values = LDA_feature(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [5, 30, 60, 75]\n",
    "samples=25\n",
    "fig, axs = plt.subplots(1, len(dimensions))\n",
    "for index, d in enumerate(dimensions):\n",
    "    X = np.random.randn(3*samples,d)\n",
    "    Y = np.repeat([1,2,3], samples)\n",
    "    vectors, values = LDA_feature(X, Y)\n",
    "    axs[index].scatter(values[:,0], values[:,1], c=Y)\n",
    "    axs[index].set_title(f\"Dimensions: {d}\")\n",
    "    axs[index].figure.set_size_inches(15, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "vecs, vals = LDA_feature(X_train, Y_train)\n",
    "plt.scatter(vals[:, 0], vals[:, 1], c=Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "vals = X_test @ vecs\n",
    "plt.scatter(vals[:, 0], vals[:, 1], c=Y_test)"
   ]
  },
  {
   "source": [
    "## 9 b)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Proof by induction\n",
    "\n",
    "\\begin{align*}\n",
    "& \\text{Let $C = {<w,x> + b}$ be the set of all binary linear classifiers and d their dimensions} \\\\\n",
    "& d = 1:\\\\\n",
    "& \\text{If we chose one data point of dimensionality 1 as the origin, at most one data point of dimensionality 1 can be classified. If n = 2, a classifier of dimensionality 3 is needed at least (XOR-Problem)}\\\\\n",
    "& \\Rightarrow n \\leq d + 1 \\\\\n",
    "& n + 1:\\\\\n",
    "& \\text{If we look at the bitcode of the data points of dimensionality n+1, we need for classifying the first n bits correctly classifiers with dimensionality n+1. For classifiying the last bit correctly as well we need another dimension of the classifer: d + 2}\\\\\n",
    "& \\text{with a linear classifier of dimension of d + 1 at most n can be classified correctly.} \\\\ \n",
    "\\end{align*}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}